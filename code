import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import csv as csv
import matplotlib.pyplot as plt

## ======= WORK ON DATA =====================================

train = pd.read_csv("BD.csv", delimiter = ';')

train = train.drop(['Id', 'ID collecteur'], axis = 1)

# RASSEMBLEMENT DES COLONNES REDONDANTES 

# - Fréquence binge drinking

train.loc[train['frqb1'] == 1, 'binge'] = 1
train.loc[train['frqb2'] == 2, 'binge'] = 2
train.loc[train['frqb3'] == 3, 'binge'] = 3
train.loc[train['frqb6'] == 4, 'binge'] = 4
train.loc[train['frqb10'] == 5, 'binge'] = 5

train = train.drop(['frqb1', 'frqb2', 'frqb3', 'frqb6', 'frqb10'], axis = 1)

# - Type logement

train.loc[train['logwho2'] == 1, 'logwho'] = 1
train.loc[train['logwho3'] == 2, 'logwho'] = 2
train.loc[train['logwho5'] == 4, 'logwho'] = 4
train.loc[train['logwho4'] == 3, 'logwho'] = 3
     
train = train.drop(['logwho2', 'logwho3', 'logwho4', 'logwho5'], axis = 1)


# - Création du paramètre consommation:

train['Conso'] = 0

train.loc[(train['max1occ'] > 0) & (train['max1occ'] < 5), 'max1occ'] = 1
train.loc[(train['max1occ'] > 5) & (train['max1occ'] < 10), 'max1occ'] = 2
train.loc[(train['max1occ'] > 10) & (train['max1occ'] < 15), 'max1occ'] = 3
train.loc[(train['max1occ'] > 15) & (train['max1occ'] < 20), 'max1occ'] = 4
train.loc[(train['max1occ'] > 20), 'max1occ'] = 5

train['Total Conso'] =  train['sixvr'] + train['nbvrtyp'] + train['max1occ'] + train['binge'] + train['frqoh']

train.loc[(train['Total Conso'] <= 13), 'Conso'] = 0
train.loc[(train['Total Conso'] > 13), 'Conso'] = 1

train = train.drop(['Total Conso', 'max1occ', 'sixvr', 'nbvrtyp', 'binge', 'frqoh'], axis = 1)



## ============== TRI DES DONNEES ========================

# - Analyse colonne par colonne

for column in train.columns:
    print (column)
    valeurs, occurences = np.unique(train[column], return_counts=True)
    print (valeurs)
    print(occurences)

# - Catégories d'age

train['age'].fillna(train['age'].median(), inplace = True)

train.loc[(train['age'] > 15) & (train['age'] <= 18), 'age'] = 1
train.loc[(train['age'] > 18) & (train['age'] <= 24), 'age'] = 2
train.loc[(train['age'] > 24) & (train['age'] <= 30), 'age'] = 3
train.loc[ train['age'] > 30, 'age'] = 4

# Remplacement des NaN restants par la moyenne 

colonnes = train.columns[:]

for colonne in colonnes:
    train[colonne].fillna(int(train[colonne].median()), inplace = True)
    

## =================================== CONVERSION EN MATRIX + CROSS VALIDATION ====================================

# ---------- Version A : utilisation de k-fold ------------------

# Conversion DataFrame to matrix
 
trainY = np.array(train.Conso)
train = train.drop(['Conso'], axis = 1)
trainX = np.array(train)
N_train = len(train_X)

# Cross validation

from sklearn import cross_validation

kf = cross_validation.KFold(train.shape[0], n_folds = 4)      #on trouve des résultats bien différents en fct de n_folds (choix de la taille du train/test)

for trainIndex, testIndex in kf:
    train_X = trainX[trainIndex]
    test_X = trainX[testIndex]
    train_Y = trainY[trainIndex]
    test_Y = trainY[testIndex]
	
print(train_X.shape[0], test_X.shape[0])


# ------------Version B: travail sur une partie des data ----------------

test = train[3001 : 6002]
train = train[ : 3001]

# Conversion DataFrame to matrix 

train_Y = np.array(train.Conso)
train = train.drop(['Conso'], axis = 1)
train_X = np.array(train)
N_train = len(train_X)

test_Y = np.array(test.Conso)
test = test.drop(['Conso'], axis = 1)
test_X = np.array(test)
N_test = len(test_X)
	

## ======= PREDICTION =======================================================



from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.feature_selection import RFE
from sklearn.naive_bayes import BernoulliNB
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Choice of model and test on train data


from sklearn.preprocessing import Imputer
my_imputer = Imputer()

train_X = my_imputer.fit_transform(train_X)
test_X = my_imputer.transform(test_X)


# ====================== LDA ====================================

clf = LinearDiscriminantAnalysis()

# ======================= AdaBoost ==============================

#
#n = 5
#learning_rate = [0.01, 0.05, 0.1, 0.2]      #tous id sauf 0.01 moins bien
#max_depth = [2, 3, 10, 20, 50]              # 10
#n_estimators = [20, 50, 80, 100, 200]       #100
#model = AdaBoostClassifier()
#model.fit(train_X, train_Y)
#prediction = model.predict(test_X)
#correct = 0	
#for i in range(len(test_Y)):
#    if prediction[i] == test_Y[i]:
#        correct += 1
#   
#print ('Accuracy: ' + str(float(correct)/(test_Y.size)))

## ====== Logistic Regression ================================================

# Selected features
Features = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #number of features selected
Accuracy = []

for f in Features:
    selector = RFE(LogisticRegression(), f, step=0.25).fit(train_X, train_Y)
    train_X_red = selector.transform(train_X)
    test_X_red = selector.transform(test_X)
    classifier = LogisticRegression(max_iter = 10000).fit(train_X_red, train_Y)
    prediction = classifier.predict(test_X_red)
    correct = 0	
    for i in range(len(test_Y)):
        if prediction[i] == test_Y[i]:
            correct += 1
       
    print ('Accuracy: ' + str(float(correct)/(test_Y.size)))
    Accuracy.append(float(correct)/test_Y.size)

plt.figure()
lw = 2
plt.plot(Features, Accuracy, color='darkorange', lw=lw)
plt.xlabel('Features selected')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. the number of features')
plt.show()

f_max = np.argmax(Accuracy) + 1
print(f_max)
selector = RFE(LogisticRegression(), f_max, step=0.25).fit(train_X, train_Y)
Health_Determinant = []
for i in selector.get_support(True):
    Health_Determinant.append(list(train.columns)[i])
print(Health_Determinant)
